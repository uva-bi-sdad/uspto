---
title: "Expand from a Seed Set"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Expand from a Seed Set}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

*Built with R 
`r getRversion()`*

***

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 5,
  fig.width = 8.84,
  dev = "CairoSVG",
  fig.ext = "svg"
)
library(uspto)
library(lingmatch)
library(knitr)
outDir <- "eda/seed_expand/"
if (!dir.exists(outDir)) outDir <- "../eda/seed_expand/"
```

```{css, echo = FALSE}
td {white-space: nowrap}
```

This example looks at a set of patent applications based on an initial seed set.

## Data Collection

### Setup

Before starting, we'll need to load the package, and point to a directory where we'd like things saved:
```r
# install if needed: remotes::install_guthub("uva-bi-sdad/uspto")
library(uspto)
outDir <- "eda/seed_expand/"
```

### Initial Search

We'll start by identifying a small set of applications with a meteorology Cooperative Patent Classification (CPC) class,
and that mention "global warming" or "climate change" in their abstracts:
```{r}
seed_set <- uspto_search(
  'g01w.cpcl. AND ("global warming".ab. "climate change".ab.)',
  "US-PGPUB",
  outFile = paste0(outDir, "seed_set.csv")
)
```

This gives us `r nrow(seed_set)` applications from between `r min(as.Date(seed_set$datePublished))`
and `r max(as.Date(seed_set$datePublished))`.

### Examination Expansions

For some context around the examination of these applications, we'll want to collect all of the other applications
examined by the examiners of the initial set. Examiners are not included in applications,
so we'll need to identify them by the office actions associated with the applications.

#### Seed Office Actions

First, we need to know which examiners examined the seed set of applications:
```{r}
office_actions <- download_office_actions(
  paste0("patentApplicationNumber:(", paste(
    sub("/", "", seed_set$applicationNumber, fixed = TRUE),
    collapse = " "
  ), ")"),
  paste0(outDir, "seed_set_office_actions.json.xz"),
  verbose = FALSE
)
```

From these `r length(office_actions)` office actions, we can get a set of examiners:
```{r}
examiners_ids <- unique(vapply(office_actions, function(oa) trimws(oa$examinerEmployeeNumber[[1]]), ""))
```

And now we have `r length(examiners_ids)` examiners who examined at least one application
in our seed set.

#### Examiner-Based Expansion

Next, we want to collect all of the other applications our identified examiners examined.
Again, since examiners are not directly associated with applications, we'll have to first collect
all of their office actions:
```{r}
all_office_actions <- download_office_actions(
  paste0("examinerEmployeeNumber:(", paste(examiners_ids, collapse = " "), ")"),
  paste0(outDir, "all_office_actions.json.xz"),
  verbose = FALSE
)
all_applications <- unique(vapply(all_office_actions, function(oa) oa$patentApplicationNumber[[1]], ""))
```

This gives us `r length(all_applications)` total applications. 

### Collect Application Data

There are at least three sources of information that might be of interest when considering the examination
of a patent application: One we have in office actions, another similar to those are the full
prosecution histories, and finally, the text of the applications themselves.

#### Prosecution Histories

Prosecution histories track when happens to an application between the submitter and the patent office.
These are recorded in the Patent Examination Data System (PEDS):

```{r}
library(jsonlite)
examinations_file <- paste0(outDir, "examination_records.json.xz")
if (file.exists(examinations_file)) {
  examination_records <- read_json(examinations_file)
} else {
  # have to break up the calls based on some limit on query length theoretically
  n <- length(all_applications)
  filters <- lapply(
    split(sub("/", "", all_applications, fixed = TRUE), sort(rep_len(seq_len(ceiling(n / 750)), n))),
    function(set) paste0("applId:(", paste(set, collapse = " "), ")")
  )
  examination_records <- unlist(lapply(filters, function(f) {
    download_peds(filters = list(f), verbose = FALSE)
  }), FALSE, FALSE)
  con <- xzfile(examinations_file)
  write_json(examination_records, con, auto_unbox = TRUE)
  close(con)
}
```

### Application Text

Finally, we can collect the actual content of each application, based on the document IDs included in
the examination records:
```{r}
document_ids <- unique(unlist(lapply(examination_records, function(r) {
  r$patentCaseMetadata$patentPublicationIdentification$publicationNumber
})))
oriDir <- "eda/original/"
if (!dir.exists(oriDir)) oriDir <- "../eda/original/"
applications <- uspto_download(document_ids, outDir = paste0(oriDir, "applications"))
```

## Analysis

We can start with a simple look at the text to get a feel for what sets our seed set apart:
```{r}
# identify the applications that were part of the initial seed set
applications$seed <- applications$applicationNumber %in% seed_set$applicationNumber

# count up terms used within abstracts
library(lingmatch)
dtm <- lma_dtm(applications$abstractHtml, "function", dc.min = 2)

# identify the terms that most identify the seed set
term_seed_sim <- lma_simets(t(dtm), applications$seed, metric = "cosine")
term_seed_sim <- sort(term_seed_sim, TRUE)[1:45]
```

```{r}
library(splot)
splot(
  term_seed_sim ~ names(term_seed_sim),
  type = "bar", sort = FALSE, title = FALSE, labx = FALSE, laby = "Similarity with Seed"
)
```

## Examination Timing

Of more interest might be how long aspects of examination takes, and what features
of the application might be associated with that time.

We can extract timing from the examination records:
```{r}
examination_timing <- do.call(rbind, lapply(examination_records, function(r) {
  times <- extract_event_timing(r$prosecutionHistoryDataBag$prosecutionHistoryData)
  data.frame(
    Classification_Time = if (!is.null(times[[1]])) times[[1]]$days[[1]] else NA,
    Assignment_Time = if (!is.null(times[[2]])) times[[2]]$days[[1]] else NA,
    First_Action_Time = if (!is.null(times[[3]])) times[[3]]$days[[1]] else NA,
    Average_Action_Time = if (!is.null(times[[3]])) mean(times[[3]]$days) else NA,
    Number_of_Actions = if (!is.null(times[[3]])) nrow(times[[3]]) else 0,
    row.names = paste0(
      substring(r$patentCaseMetadata$applicationNumberText$value, 1, 2),
      "/",
      substring(r$patentCaseMetadata$applicationNumberText$value, 3)
    )
  )
}))
examination_timing <- examination_timing[applications$applicationNumber, ]

# look at only applications with extracted times
has_timings <- which(!is.na(examination_timing$First_Action_Time))
examination_timing <- examination_timing[has_timings, ]
```

Then look at the relationship between timings and actions:
```{r}
cors <- lma_simets(t(examination_timing), metric = "pearson")
colnames(cors) <- seq_len(ncol(cors))
rownames(cors) <- paste0("(", colnames(cors), ") ", rownames(cors))
round(cors, 3)
```

Or associate timing and number of actions with terms used in the application's abstract:
```{r}
# align applications with valid timings
term_time_sim <- lma_simets(t(dtm[has_timings, ]), t(examination_timing), metric = "pearson")
```

```{r, fig.height=3}
for (aspect in colnames(term_time_sim)) {
  terms <- term_time_sim[, aspect]
  terms <- sort(terms, TRUE)[1:45]
  splot(
    terms ~ names(terms),
    title = paste0("Terms Most Associated With ", gsub("_", " ", aspect, fixed = TRUE)),
    type = "bar", sort = FALSE, laby = "Pearson's r", labx = FALSE
  )
}
```

## Art Unit Topics

We can also get a feel for what seems to characterize Group Art Units by extracting topics from the
applications they review.

We can start by getting art unit information for each application:
```{r}
artunits <- unlist(lapply(examination_records, function(r) {
  res <- r$patentCaseMetadata$groupArtUnitNumber$value
  names(res) <- sub("(\\d{2})", "\\1/", r$patentCaseMetadata$applicationNumberText$value, perl = TRUE)
  res
}))[applications$applicationNumber]

# number of applications assigned to each art unit
table(artunits)
```

Then, we could get the same sort of associated set of terms, as with the seed set:
```{r}
gaus <- unique(artunits)
term_artunit <- lma_simets(
  t(dtm), vapply(artunits, "==", logical(length(gaus)), gaus),
  metric = "pearson"
)
colnames(term_artunit) <- gaus

# look at top 10 terms for each
get_top_terms <- function(loadings, n = 10) {
  kable(as.data.frame(vapply(colnames(loadings), function(col) {
    rownames(loadings)[order(loadings[, col], decreasing = TRUE)[seq_len(n)]]
  }, character(n))))
}
get_top_terms(term_artunit)

# we could use this to roughly predict art unit in-sample:
predicted_art_unit <- gaus[max.col(dtm %*% term_artunit)]
round(table(artunits, predicted_art_unit) / as.numeric(table(artunits)), 4)
```

We could also extract general topics from this set of applications, then see how
much they are represented in each art unit's assigned applications:
```{r}
# weigh terms
wdtm <- lma_weight(dtm, "tf-idf", FALSE)

# get 10 simple topics
## remotes::install_github("miserman/lusilab")
library(lusilab)
loadings <- taffyInf(wdtm, 10)
colnames(loadings) <- paste0("taffy_", seq_len(ncol(loadings)))
scores <- dtm %*% loadings
get_top_terms(loadings)
# compare with kmeans
topics_km <- kmeans(wdtm, 10)
loadings_km <- t(topics_km$centers)
dimnames(loadings_km) <- list(colnames(dtm), paste0("km_", seq_len(ncol(loadings))))
scores_km <- dtm %*% loadings_km
get_top_terms(loadings_km)
# compare with NMF
library(RcppML)
topics_nmf <- nmf(wdtm, 10, verbose = FALSE)
loadings_nmf <- t(topics_nmf$h)
dimnames(loadings_nmf) <- list(colnames(dtm), paste0("nmf_", seq_len(ncol(loadings_nmf))))
colnames(topics_nmf$w) <- colnames(loadings_nmf)
get_top_terms(loadings_nmf)
# see how similar topics are in terms of loadings
loadings_all <- cbind(loadings, loadings_km, loadings_nmf)
loading_cors <- lma_simets(t(loadings_all), metric = "pearson")
topic_names <- rownames(loading_cors)
scores_all <- dtm %*% loadings_all

display_matrix <- function(l, threshold = .1) {
  v <- as.data.frame(as.matrix(l))
  v[abs(v) < threshold] <- 0
  kable(v, digits = 3, format.args = list(zero.print = "."))
}

display_matrix(loading_cors[grep("km", topic_names), grep("taffy", topic_names)])
display_matrix(loading_cors[grep("nmf", topic_names), grep("taffy", topic_names)])
display_matrix(loading_cors[grep("nmf", topic_names), grep("km", topic_names)])
```

Now we can look at trends in these topics over time within art unit:
```{r}
applications$year <- as.numeric(applications$datePublYear)
trends <- vapply(gaus, function(au) {
  su <- artunits == au
  lma_simets(t(scores_all[artunits == au, ]), applications[su, "year"], metric = "pearson")
}, numeric(ncol(scores_all)))
display_matrix(t(trends), .1)
focal_unit <- names(which.max(colMeans(abs(trends))))
focal_topics <- names(which(apply(abs(trends), 1, max) > .4))
get_top_terms(loadings_all[, focal_topics])
splot(
  scores_all[, focal_topics] ~ applications$year,
  su = artunits == focal_unit, mv.scale = TRUE, sud = FALSE, myl = c(-2, 9),
  title = paste("Topic Over Time in Art Unit", focal_unit),
  lines = "spline", laby = "Topic Use (z-scored)", labx = FALSE
)
```
